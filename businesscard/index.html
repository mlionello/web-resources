<!DOCTYPE html>
<html lang="en">
<head>
<meta name="google-site-verification" content="5j0-UcrKcxTQC6iqlw6WamWdOdtHEqzlvxeHXHeHXKk">
<meta name="description" content="Matteo Lionello, Deep Learning and Machnine Learning developer">
<meta name="robots" content="index, follow">
<meta name="googlebot" content="index, follow">
<title>Matteo Lionello</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="styles/style_general.css">
    <link rel="stylesheet" href="styles/style_navmenu.css">
    <link rel="stylesheet" href="styles/style_low.css">
    <link rel="stylesheet" href="styles/style_mid.css">
    <link rel="stylesheet" href="styles/slider0.css">
    <link rel="stylesheet" href="styles/media.css">

<meta charset="utf-8">
</head>

<body>
<script src="scripts/mscript.js">
</script>

<!-- Top Navigation Menu -->
<div class="topnav">
  <a href="#" class="active" id="menupointer">Home</a>
  <!-- Navigation links (hidden by default) -->
  <div id="myLinks">
    <a href="#" onclick="turnpage('cv'); showmmenu();">Home</a>
    <a href="#" onclick="turnpage('cv'); showmmenu();">CV</a>
    <a href="#" onclick="turnpage('projects'); showmmenu();">Projects</a>
    <a href="#" onclick="turnpage('publications'); showmmenu();">Publications</a>
    <a href="#" onclick="turnpage('sum'); showmmenu();">About me</a>
    <a href="#" onclick="turnpage('news'); showmmenu();">News</a>
    <a href="#" onclick="turnpage('contacts'); showmmenu();">Contacts</a>
  </div>
  <!-- "Hamburger menu" / "Bar icon" to toggle the navigation links -->
  <a href="#" class="icon" onclick="showmmenu();">
    <i class="fa fa-bars"></i>
  </a>
</div>

<span id="undercons" style="pointer-events: none;"><img width="200px" src="images/undercons.png"></span>

<header >
<span id="topmenu"><ul>
<li class="icons"><a target="_blank" href="https://www.researchgate.net/profile/Matteo-Lionello"><img src="images/researchgateicon.png"></a></li>
<li class="icons"><a target="_blank" href="https://linkedin.com/in/matteo-lionello-393756110"><img src="images/linkedinicon.png"></a></li>
</ul></span>

<nav id="navbar">
<ul id="navlist">
<li id="navhome" class="navopen"><a class="mainnav" href="#" onclick="turnpage('home');return false;">HOME</a></li>
<li id="navcv"><a class="mainnav" href="#" onclick="turnpage('cv');return false;">CV</a></li>
<li id="navprojects"><a class="mainnav" href="#" onclick="turnpage('projects');return false;">PROJECTS</a></li>
<li id="navpublications"><a class="mainnav" href="#" onclick="turnpage('publications');return false;">PUBLICATIONS</a></li>
<li id="navsum"><a class="mainnav" href="#" onclick="turnpage('sum');return false;">ABOUT ME</a></li>
<li id="navcontacts"><a class="mainnav" href="#" onclick="turnpage('contacts');return false;">CONTACTS</a></li>
<li id="navnews"><a class="mainnav" href="#" onclick="turnpage('news');return false;">NEWS</a></li>
</ul>
</nav>
</header>

<main id="mmain">
<section id="home" class="firstslide" style="text-align:center;
order: 1">
<h1> Matteo Lionello </h1>
<p>Research and Development in Machine Learning & Deep Learning Technologies for Audio and Sound 
</section>

<section id="cv" class="content" style=" order: 2">
    <div class="showmeinmobile"><h1 style="text-align:center;">Curriculum</h1><hr></div>
<div id="downloadidwrapperdiv" style="font-family:navfont1; float:right; border:2px; font-size:large;"><a href="matteolionello_cv.pdf"  download target="_blank"><button class="btn"><i class="fa fa-download"></i> download</button></a></div>
<div class="cvrsection">
    <h1>Education and Job Experience</h1><hr>
<div style="padding-top:20px"><b> Machine Learning research consultancy for audio , UK</b> <span style="float:right">
<i>2021 - present</i></span></div></div>

<div style="padding-top:20px"><b> The Bartlett Institute, University Collage London (UCL), London </b> <span style="float:right">
<i>October 2018 - February 2021</i></span><br>
MPhil Student at the Bartlett Institute<br>
<div style="padding-left:20px; padding-right:15%">Thesis title: <i>A new methodology for modelling urban soundscapes: a psychometric revisitation of the current standard and a Bayesian approach for individual response prediction</i></div></div>

<div style="padding-top:20px"><b>  Universitat Pompeu Fabra, Barcelona </b> <span style="float:right">
<i> September - December 2017</i></span><br>
Visiting student at Music Technology Group, MTG - TELMI ERC project<br>
<div style="padding-left:20px; padding-right:15%">Project title: <i>A Machine Learning Approach to Violin Vibrato Modelling in Audio Performances and a Didactic Application for Mobile Devices</i></div></div>

<div style="padding-top:20px"><b>  Aalborg University, Copenhagen </b> <span style="float:right">
<i>   October 2016 - June 2018</i></span><br>
MSc in Sound and Music Computing, SMC<br>
<div style="padding-left:20px; padding-right:15%">Thesis title: <i>Deep Learning for Sounds Representation and Generation</i></div></div>

<div style="padding-top:20px"><b> University of Padova, Padova</b> <span style="float:right">
<i>September 2011 - November 2015</i></span><br>
BSc in Information Engineering <br>
<div style="padding-left:20px; padding-right:15%">Thesis title: <i>Interactive Soundscapes: Design of a physical space augmented by dynamic sound rendering.</i></div>
</div>


<div class="cvrsection"><h1>Reviewer activity for Journals:</h1><hr>
<ul><li style="padding-top:10px"> Neurocomputing, Elsevier</li>
<li style="padding-top:10px"> Quality and Quantity, Springer Science+Business Media</li>
<li style="padding-top:10px"> Building Simulation Journal, Springer</li>
</ul></div>

<div class="cvrsection"><h1>Conferences and Workshops</h1><hr>
<ul name="cv-conf-list" style="list-style-type: none;"><li>Digital Music Research Network One-day Workshop 2020 (DMRN+15) <span style="float:right"> <i>December 15, 2020 London</i></span></li>
<li style="padding-top:10px">International Conference on Acoustics, Speech, and Signal Processing (ICASSP) <span style="float:right"> <i> May 4 to 8, 2020 Barcelona</i></span></li>
<li style="padding-top:10px">International Congress on Acoustics (ICA2019) <span style="float:right"> <i> September 9 to 13, 2019 Aachen</i></span></li>
<li style="padding-top:10px">Machine Learning for Acoustics Summer School (UKANSS19) <span style="float:right"> <i> August 5 to 9, 2019 Gregynog Hall, Tregynon</i></span></li>
<li style="padding-top:10px">Soundscape Workshop (IOA) <span style="float:right"> <i> June 25, 2019 London</i></span></li>
<li style="padding-top:10px">Sound and Music Computing Conference (SMC Conference) <span style="float:right"> <i> July 4 to 7, 2018 Limassol</i></span></li>
<li style="padding-top:10px">International Workshop on Machine Learning and Music (MML) <span style="float:right"> <i> October 6, 2017 Barcelona</i></span></li>
<li style="padding-top:10px">New Interfaces for Musical Expression (NIME) <span style="float:right"> <i> May 15 to 19, 2017 Copenhagen</i></span></li>
<li style="padding-top:10px">Sound and Music Computing Summer School <span style="float:right"> <i> August 26 to 30, 2016 Hamburg</i></span></li>
</ul></div>

<div class="cvrsection"><h1>Prizes and Awards</h1><hr>
<ul>
<li style="padding-top:10px">UKAN Acoustics Network Summer School Grant</li>
<li style="padding-top:10px">pounds 500,00 Young Scientist Conference Attendance Award</li>
<li style="padding-top:10px">Winner of pounds 21,837 studentship (full time) in urban sound environment, EU ERC.</li>
<li style="padding-top:10px">Awarded grant of DKK 6.963,00 by OTTO MØNSTEDS FOND.</li></ul>
</div>

<div class="cvrsection"><h1>Technical Knowledge</h1><hr>
<table>
<tr><td>Programming Languages: </td><td> Java, Python, Matlab, Unix,  basic knowledge of C++ and PhP</td></tr>
<tr><td>Libraries: </td><td> Keras, Tensorflow, Pytorch, Pytorch Lightning</td></tr>
<tr><td>Protocols: </td><td> OpenSoundsControl (OSC) and TCP/IP</td></tr>
<tr><td>Databases: </td><td> basic knowledge of SQL and MySQL </td></tr>
<tr><td>Tools: </td><td> PureData, Android, HTML, CSS and basic knowledge of PhP</td></tr>
</table>
</div>

<div class="cvrsection" style="margin-bottom:100px"><h1>Language Skills</h1><hr>
<i>Italian:</i> Native language; 
<i>English:</i> C1 (7.5 IELTS, October 2018); <i>Spanish:</i>  B\'asico
</div>
</section>

<section id="projects" class="content"  style=" order: 3">
<div class="showmeinmobile"><h1 style="text-align:center;">Projects</h1><hr></div>
    
<div  style="margin-bottom:100px">    
<h1>Audio Enhancement - real time</h1>
<h3>Audio quality enhancement</h3>
<p>This work was part of a project developed for a private company</p>
<h3>Real time audio denoising</h3>
<p>This work was part of a project developed for a private company</p>

    
<h1>Urban Soundscape index modelling</h1>
<h3>Bayesian-NN for individual soundscape assessment predictions and a perceptual index design, development, evaluation</h3>
<p>A Bayesian modelling approach has been implemented to represent and analyse the uncertainty highlighted in the psychometric revisitation of the current standard soundscape ISO. </p>
<p>According to some commonly agreed criteria to u</p>
<h3>Psychometric re-visitation of the current standard ISO for soundscape measuring and data collection protocol design</h3>
<p>address: University College London, London <br>link: <a target="_blank" href="https://www.frontiersin.org/articles/10.3389/fpsyg.2020.602831/full">Pyscometric revistation of ISO protocol</a> <br>link: <a target="_blank" href="https://www.mdpi.com/2076-3417/10/7/2397">Data collection protocol design</a> </p>
<p>Likert scales are useful for collecting data on attitudes and perceptions from large samples of people. In particular, they have become a well-established tool in soundscape studies for conducting in situ surveys to determine how people experience urban public spaces. However, it is still  <button class="expandable" style="color: gray; border:0;"> <b><i><u> <a>... read more </a></u></i></b></button></p>
    <p style="display: none"> Likert scales are useful for collecting data on attitudes and perceptions from large samples of people. In particular, they have become a well-established tool in soundscape studies for conducting in situ surveys to determine how people experience urban public spaces. However, it is still  unclear whether the metrics of the scales are consistently interpreted during a typical assessment task. The current work aims at identifying some general trends in the interpretation of Likert scale metrics and introducing a procedure for the derivation of metric corrections by analyzing a case study dataset of 984 soundscape assessments across 11 urban locations in London. According to ISO/TS 12913-2:2018, soundscapes can be assessed through the scaling of 8 dimensions: pleasant, annoying, vibrant, monotonous, eventful, uneventful, calm, and chaotic. The hypothesis underlying this study is that a link exists between correlations across the percentage of assessments falling in each Likert scale category and a dilation/compression factor affecting the interpretation of the scales metric. The outcome of this metric correction value derivation is introduced for soundscape, and a new projection of the London soundscapes according to the corrected circumplex space is compared with the initial ISO circumplex space. The overall results show a general non-equidistant interpretation of the scales, particularly on the vibrant-monotonous direction. The implications of this correction have been demonstrated through a Linear Ridge Classifier task for predicting the London soundscape responses using objective acoustic parameters, which shows significant improvement when applied to the corrected data. The results suggest that the corrected values account for the non-equidistant interpretation of the Likert metrics, thereby allowing mathematical operations to be viable when applied to the data. - From the abstract<span class="collapsable" style="color: gray; "> <b><i><u> <a> show less </a></u></i></b></span></p>
    
<h1>Music recommendation system</h1>
<h3>Parametric t-SNE (ANN) for music recommendation system, playlist generation and browser GUI for online music streaming providers</h3>
<p>address: Aalborg University, Copenhagen<br>
    link: <a target="_blank" href="https://vimeo.com/273294798"> VIMEO </a></p>
<p>
    This project presented the development of the model and a user interface for a music-space exploration based on the t-SNE dimension reduction technique, aiming at <button class="expandable" style="color: gray; border:0;"> <b><i><u> <a>... read more </a></u></i></b></button></p>
    <p style="display: none"> This project presented the development of the model and a user interface for a music-space exploration based on the t-SNE dimension reduction technique, aiming at preserving the shapes and structure of a high dimensional dataset of songs, dictated by N-dimensional features vector, to its projection onto a plane. We investigate different models obtained from using different structures of hidden layers, pre-training technique, features selection, and data pre-processing. The resulting output model has been used to build a music-space of 20000 songs, visually rendered for browser interaction, providing the user a certain degree of freedom to explore it by changing the features to highlight, offering an immersive experience for music exploration and playlist generation.<span class="collapsable" style="color: gray; "> <b><i><u> <a> show less </a></u></i></b></span></p>

<h1>Deep Learning for Music</h1>
<h3>Violin Vibrato modelling (SVM) and GUI on android rendering with real time pitch detection</h3>
<p>address: MTG Pompeu Fabra Universitat, Barcellona
link: <a target="_blank" href="https://www.researchgate.net/publication/326331257_A_Machine_Learning_Approach_to_Violin_Vibrato_Modelling_in_Audio_Performances_and_a_Didactic_Application_for_Mobile_Devices"> A Machine Learning Approach to Violin Vibrato Modelling in Audio Performances and a Didactic Application for Mobile Devices</a><br>link
<a target="_blank" href="https://vimeo.com/276617353">VIMEO</a></p>
<p>We present a machine learning approach to model vibrato in classical music violin audio performances. A set of descriptors have been extracted from the music scores of the performed pieces and used to train a model for classifying notes into vibrato or non-vibrato, as well as for predicting the performed vibrato amplitude and frequency. <button class="expandable" style="color: gray; border:0;"> <b><i><u> <a>... read more </a></u></i></b></button></p>
    <p style="display: none">We present a machine learning approach to model vibrato in classical music violin audio performances. A set of descriptors have been extracted from the music scores of the performed pieces and used to train a model for classifying notes into vibrato or non-vibrato, as well as for predicting the performed vibrato amplitude and frequency. In addition to score features we have included a feature regarding the fingering used in the performance. The results show that the fingering feature affects consistently the prediction of the vibrato amplitude. Finally, an implementation of the resulting models is proposed as a didactic real-time feedback system to assist violin students in performing pieces using vibrato as an expressive resource. <span class="collapsable" style="color: gray; "> <b><i><u> <a> show less </a></u></i></b></span> </p>

<h3>LSTM modelling for melody and rhythmic structure extraction and generation</h3>
<p>address: Aalborg University, Copenhagen</p>
<p>One of the most suggestive concept in artificial intelligence applied to music, shown in several recent studies, is the concept of style. Even if the definition of style is hard to be explicated without considering also historical and social contexts, from a basic overview we can think about it as the sequence of patterns composing the structure of a music piece. The style could be thought indeed as a particular pattern hidden <button class="expandable" style="color: gray; border:0;"> <b><i><u> <a>... read more </a></u></i></b></button></p>
    <p style="display: none">One of the most suggestive concept in artificial intelligence applied to music, shown in several recent studies, is the concept of style. Even if the definition of style is hard to be explicated without considering also historical and social contexts, from a basic overview we can think about it as the sequence of patterns composing the structure of a music piece. The style could be thought indeed as a particular pattern hidden  in a sequence of symbols describing a music work, doing so it will be easy to manage the information represented by the structure containing the symbols, using them in order to manipulate the style itself or to replicate it.
Making a deeper step into this subject we can separate the main goal in two different problems. One first problem is, given an audio support from which extract the style information, find an automatic way to reduce the original audio to a stream of symbols in which the style is encoded. The second problem concerns using this stream of symbols to decode the style they are carrying; in order to do that it is introduced a support structure capable to decode and memorize the piece structure that will be re-used to compose new music sequences referring to the same style of the original one.
The purpose of this study is the evaluation of Long Short Term Memory network, for music generation from a percussive sequence as an example. The sequence segmented and symbolized through a single-linkage algorithm based on MFCC analysis, and the fed into the network. Then the network is trained with the analyzed data and gains the ability to generate new percussive sequences, according to the example. The results are compared with the previously implemented method of Variable Length Markov Chain models for music generation. <span class="collapsable" style="color: gray; "> <b><i><u> <a> show less </a></u></i></b></span> </p>
    
<h3>Variational Autoencoder for sounds morphing</h3>
<p>address: Aalborg university<br>
link: <a target="_blank" href="https://www.researchgate.net/publication/336847453_End-To-End_Dilated_Variational_Autoencoder_with_Bottleneck_Discriminative_Loss_for_Sound_Morphing_-_A_Preliminary_Study"> End-To-End_Dilated Variational Autoencoder with Bottleneck Discriminative Loss for Sound Morphing</a></p><p>
    <p>This project was developed with tensorflow 1.4 where probabilistic inference was not yet available. Two strategies for end-to-end variational autoencoders (VAE) for sound morphing are compared: VAE with  <button class="expandable" style="color: gray; border:0;"> <b><i><u> <a>... read more </a></u></i></b></button></p>
    <p style="display: none"> This project was developed with tensorflow 1.4 where probabilistic inference was not yet available. Two strategies for end-to-end variational autoencoders (VAE) for sound morphing are compared: VAE with dilation layers (DC-VAE) and VAE only with regular convolutional layers (CC-VAE). The training strategy used a combination of the following loss functions: 1) the time-domain mean-squared error for reconstructing the input signal, 2) the Kullback-Leibler divergence to the standard normal distribution in the bottleneck layer, and 3) the classification loss calculated from the bottleneck representation. On a database of spoken digits, 1-nearest neighbor classification was used to show that the sound classes separate in the bottleneck layer. We introduce the Mel-frequency cepstrum coefficient dynamic time warping (MFCC-DTW) deviation as a measure of how well the VAE decoder projects the class center in the latent (bottleneck) layer to the center of the sounds of that class in the audio domain. In terms of MFCC-DTW deviation and 1-NN classification, DC-VAE outperforms CC-VAE. These results limited to the current parametrization and the dataset indicate that DC-VAE is more suitable for sound morphing than CC-VAE, since the DC-VAE decoder better preserves the topology when mapping from the audio domain to the latent space.<span class="collapsable" style="color: gray; "> <b><i><u> <a> show less </a></u></i></b></span></p>
    
<h1>Artistic augmented space</h1>
<h3>An Interactive Soundscape augmented space</h3>
<p>Address: University of Padua<br>
    link: <a target="_blank" href="https://vimeo.com/146137215">VIMEO</a></p>
    <p>A sonic augmented physical space through granular synthesis: this installation was inspired by Truax's Entrance to the Harbour, from The Vancouver Soundscape 1973 and Pacific Fanfare from The Vancouver Soundscape 1996. It reproduced a collage of sounds referring to a real soundscape. The sounds were spacialized in  <button class="expandable" style="color: gray; border:0;"> <b><i><u> <a>... read more </a></u></i></b></button></p>
    <p style="display: none"><button class="expandable" style="color: gray; border:0;"> <b><i><u> <a>... read more </a></u></i></b></button></p>
    <p style="display: none"> A sonic augmented physical space through granular synthesis: this installation was inspired by Truax's Entrance to the Harbour, from The Vancouver Soundscape 1973 and Pacific Fanfare from The Vancouver Soundscape 1996. It reproduced a collage of sounds referring to a real soundscape. The sounds were spacialized in a room according to the users's detected position by a camera set on the top of the room. The user had the possibility to explore both the spatial structure of the soundscape and the acoustic structure of its sounds by entering in a central area of the room, where  a granular decomposition of the sounds was applied. Launched in the late '70s at the Simon Fraser University, Soundscape Composition is a set of composite strategies working on sound environment. Traditionally linked with granular synthesis, it uses electronic music tools to elaborate environmental sounds, preserving their original contexts. The current state of the art technologies allowed the design and the development of an interactive environment inspired by soundscape composition, in which a user can explore a sound augmented reality referring to a real soundscape. The soundscape exploration occurs on two different layers: on a first layer the user can spatially explore the soundscape, while on a second layer an exploration of the structural composition of the sounds that build the soundscape takes place. This second kind of exploration happens through a granular analysis of the sounds. The user moving through the installation modifies the synthesis and sound dynamic parameters, building a cognitive structure of the augmented environment. The sound feedback of the environment modifies user's awareness and, consequently, his decisions on how to move within it.<span class="collapsable" style="color: gray; "> <b><i><u> <a> show less </a></u></i></b></span></p>
<h3>Environmental Sonification</h3>
    <p>address: Aalborg University, Copenhagen<br>
    link: <a target="_blank" href="https://www.youtube.com/watch?v=mvRMIrU0AE4"> YOUTUBE</a></p>
    <p>Competitive workshop for developing solutions to the new Lighting-Sound system at the AAU University bridge - winning project</p>
    </div>


<script>
var coll = document.getElementsByClassName("expandable");
var i;
for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.parentElement.nextElementSibling;
    content.style.display = "block";
    this.parentElement.style.display = "none";
  });
}

var coll = document.getElementsByClassName("collapsable");
var i;
for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.parentElement.previousElementSibling;
    content.style.display = "block";
    this.parentElement.style.display = "none";
  });
}
                            </script>
</section>
<section id="publications" class="content" style=" order: 4">
    <div class="showmeinmobile"><h1 style="text-align:center;">Publications</h1><hr></div>
2021:<br><br>

<u>M. Lionello</u>, F. Aletta, J. Kang, (2021) "<i>Introducing a Method for Intervals Correction on Multiple Likert Scales: A Case Study on an Urban Soundscape Data Collection Instrument</i>" Frontiers in Psychology<br><br>

2020:<br><br>

<u>M. Lionello</u>, F. Aletta, J. Kang, (2020) "<i>A systematic review of prediction models for the experience of urban soundscapes</i>" in Applied Acoustics<br><br>

A. Mitchell,  T.  Oberman, F.  Aletta,  M.  Erfanian,  M. Kachlicka,  <u>M.  Lionello</u>,  J.  Kang,  (2020) "<i>The  Soundscape  Indices  (SSID)  Protocol:  A  Method  for  Urban  Soundscape  Surveys—Questionnaires   with   Acoustical   and   Contextual   Information</i>" in Applied Sciences <br><br>

<u>M. Lionello</u>, F. Aletta, J. Kang. (2019) "<i>On the dimension and scaling analysis of soundscape assessment tools: a case study about the “Method A” of ISO/TS 12913-2:2018</i>" Invited paper of 23rd international congress on acoustics.<br><br>

2019:<br><br>

F. Aletta, T. Oberman, J. Kang, M. Erfanian, M. Kachlicka, <u>M. Lionello</u>, A. Mitchell, (2019) "<i>Associations between soundscape experience and self-reported wellbeing in open public urban spaces: a field study</i>", The LANCET,<br><br>

<u>M. Lionello</u>, H. Purwins (2019) "<i>End-To-End Dilated Variational Autoencoder with Bottleneck Discriminative Loss for Sound Morphing - A Preliminary Study</i>" DOI: 10.13140/RG.2.2.21572.58240/1.<br><br>

2018:<br><br>

<u>M. Lionello</u>, L. Pietrogrande, H. Purwins, M. Abou-Zleikha (2018)
"<i>Exploration of Musical Space with Parametric t-SNE in a Browser Interface</i>"
Proceedings to the 15th Sound and Music Computing Conference, Limassol, Cyprus.<br><br>

<u>M. Lionello</u>, R. Ramirez, (2018)
"<i>A Machine Learning Approach to Violin Vibrato Modelling in Audio Performances and a Didactic Application for Mobile Devices</i>"
Proceedings to the 15th Sound and Music Computing Conference, Limassol, Cyprus.<br><br>

<u>M. Lionello</u>, H. Purwins "<i>Deep Learning for Sounds Representation and Generation</i>" Master thesis, 2018. Aalbrog University, Copenhagen <a href="https://projekter.aau.dk/projekter/files/281073844/thesis\_matteolionello.pdf"> available here </a><br><br>

2017:<br><br>

<u>M. Lionello</u>, M. Mandanici, S. Canazza, E. Micheloni, (2017)
<i>Interactive Soundscapes: Developing a Physical Space Augmented through Dynamic Sound Rendering and Granular Synthesis</i>"
Proceedings of the 14th Sound and Music Computing Conference, Espoo, Finland.<br><br>

Complete list of publications available on my personal <a href="http://www.researchgate.net/profile/Matteo_Lionello"> Research Gate page</a>

</section>
<section id="sum" class="content"  style=" order: 5">
    <div class="showmeinmobile"><h1 style="text-align:center;">Summary</h1><hr></div>
<div style="display: inline-block;
width: 20%;
vertical-align: top;
padding: 15px;"><img width="150px" src="images/id_lowres.jpg"></div><div style="display: inline-block;
width: 70%;
padding-left: 40px;">
    <h4>Me (very briefly):</h4>
    <p>I love computers, music and all in the between. I like to dive into new challenges, to formalize problems, and to solve them by developing strategies.</p>
    
    <h4>My student journey</h4><p>I am specialized in developing machine learning and deep learning tools across multidisciplinary fields including music, sound, digital signal processing, and urban environment. I started my studies at University of Padua with a bachelor in Information Engineering. I then specialized in Sound and Music Computing at Aalborg University where I dedicated most of my time in machine learning and deep learning projects, including induxtry collaborations and visiting period at the Music Technology Group in Barcelona. I pursued a further MPhil degree at the Institute for Environmental Design and Engineering, The Bartlett, University College London where, as part of the prestigious ERC Advanced Grant in Urban Soundscape Indices (SSID), I developed machine learning methods and psychometric analysis tools to predict urban soundscape under the supervision of Prof. Jian Kang.</p>

<p>I am highly motivated and interested also in studies of ecological preservation of the environment and in the artistic and humanistic fields with the aim of using engineering tools to promote and preserve cultural heritage.</p>

</section>

<section id="contacts" class="content"  style=" order: 6">
    <div class="showmeinmobile"><h1 style="text-align:center;">Contacts</h1><hr></div>
<div style="display: inline-block; 
vertical-align: top;
padding: 15px;"><img style="filter: invert(1);" width=400px src="images/email.png"></div>
<div><span><a target="_blank" href="https://scholar.google.com/citations?user=DEXi4zsAAAAJ&hl=da" ><img title="google scholar" src="images/scholar.png"></a></span>
<span><a target="_blank" href="https://www.scopus.com/authid/detail.uri?authorId=57211749952" ><img title="scopus" src="images/scopus.png"></a></span>
<span><a target="_blank" href="https://www.linkedin.com/in/matteo-lionello-393756110/" ><img title="linkedin" src="images/linkedinicon.png"></a></span>
<span><a target="_blank" href="" ><img title="research gate" src="images/researchgateicon.png"></a></span>
<span><a target="_blank" href="https://orcid.org/0000-0003-1559-2398" ><img title="ORCID" src="images/ORCID_iD.png"></a></span></div>
</section>

<section id="news" class="content"  style=" order: 7">
    <div class="showmeinmobile"><h1 style="text-align:center;">News</h1><hr></div>

<div style="padding-bottom: 100px; padding-top: 100px; text-align:center">
<iframe style="padding-top:30px" src="https://www.linkedin.com/embed/feed/update/urn:li:share:6839094567208407040" allowfullscreen="" title="Embedded post" width="504" height="600" frameborder="0"></iframe>
<iframe style="padding-top:30px" src="https://www.linkedin.com/embed/feed/update/urn:li:share:6717780045919739904" allowfullscreen="" title="Embedded post" width="504" height="600" frameborder="0"></iframe>
<iframe style="padding-top:30px" src="https://www.linkedin.com/embed/feed/update/urn:li:share:6804076873216217088" allowfullscreen="" title="Embedded post" width="504" height="600" frameborder="0"></iframe>
<iframe style="padding-top:30px" src="https://www.linkedin.com/embed/feed/update/urn:li:share:6780839327950753792" allowfullscreen="" title="Embedded post" width="504" height="600" frameborder="0"></iframe>
<iframe style="padding-top:30px" src="https://www.linkedin.com/embed/feed/update/urn:li:share:6759445895189671936" allowfullscreen="" title="Embedded post" width="504" height="600" frameborder="0"></iframe>
<iframe style="padding-top:30px" src="https://www.linkedin.com/embed/feed/update/urn:li:share:6745304107919130625" allowfullscreen="" title="Embedded post" width="504" height="600" frameborder="0"></iframe>
</div></section>
</main>


<footer>
    <div style="position:fixed; bottom:30px; left:0; right:0; height:4px; background-color:rgb(245, 245, 200)"></div>
        <div>
            <span id="spanfooter">Matteo Lionello contact: lione***.mat***[at]gmail.com, Website template available at: <a href="http://www.github.com/mlionello/web-resources/tree/main/businesscard">github.com/mlionello</a>: forks are welcome!</span></span></div>


</footer>

</body>
</html>
